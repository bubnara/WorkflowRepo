---
title: "ISA 616 Workflow"
author: "Andrew Bubnar"
date: "10/1/2020"
output: 
  html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Optimizing Capital Bikeshare Product Distribution
##### An Analysis by Andrew Bubnar

## Overview
I believe that there is room to optimize the distribution process of Capital Bikeshare through predictive modeling. Capital Bikeshare is a bikesharing company, owned by Lyft Inc., that allows registered and casual users to rent public bikes in major cities around the world. Currently, bikes are deployed uniformly over some time dimensions. This creates a lapse in efficiency, that is, bikes are not always available when they need to be, and some bikes are sitting idle, producing no revenue. If we can predict the number of bikes that will be in use at any given time, we will maximize use, and thus profitability. 

The purpose of this analysis is, firstly, to deduce what factors impact the outcome of riders. The data that will be considered is primarily related to time, weather, and expected traffic. After concluding the impacts of each variable, suggestions to Capital Bikeshare's Logistics Controls team will be made (and passed along to distributors), to allow for a more satisfied customer base and increased profitabilty.

```{r, include = FALSE }
# Initialize all Packages that will be used
if(require(pacman)==FALSE)
  install.packages("pacman")

pacman::p_load(tidyverse, gtsummary, dplyr, DT, ggplot2, caret, corrplot, knitr)

```



## Data Selection, Description and Summary


#### Data Sourcing


The data involved was produced in conjunction by Capital Bikeshare and freemeteo.com. Two datasets were provided for analyses, differing on how they aggregate over time. One dataset has hourly data for ten independent variables with three dependent variables, while the other aggregates this same data by day. A data head, as well as a descriptive table defining each variable, can be found below:



#### Data Head & Codebook

```{r, echo = FALSE }
library(DT)
# Read in Data
daily = read.csv("day.csv")
hourly = read.csv("hour.csv")
head(hourly)

#Split data into training and validation sets
set.seed(42)
dailytrainindex = sample(1:nrow(daily), size = round(0.7*nrow(daily)), replace = FALSE)
daily.train = daily[dailytrainindex,]
daily.validation = daily[-dailytrainindex,]
hourlytrainindex = sample(1:nrow(hourly), size = round(0.7*nrow(daily)), replace = FALSE)
hourly.train = hourly[hourlytrainindex,]
hourly.validation = hourly[-hourlytrainindex,]

# Create data frame that holds descriptions
desc = as.data.frame(c('Observation #','Date','Season (1 = Spring, 2=Summer, etc.)','Year (0 = 2011, 1 = 2012)','Month (numeric)','Hour (0 = 12:00 am - 12:59 am, 23 = 11:00pm - 11:59 pm)','Public Holiday (Base 0 = no)', 'Day of Week (Sunday = 0)', 'Work Day (non-holiday, weekday)', 'Weather Condition (1 = Clear/Parly Cloudy, 2 = Mist/Cloudy, 3 = Light Snow/Rain, 4 = Heavy Rain/Snow/Thunderstorm','True Temperature (Normalized)','Feels Like Temperature (Normalized)','Windspeed (Normalized)','Humidity','Count of Unregistered Users','Count of Registered Users','Total Count of Users'),colnames(hourly), colnames='')
```

```{r }
# Display a Data Table to Display Descriptions
DT::datatable(desc, colnames="")
```

#### Data Structure

The columns are comprised of a primary key for each observation, the date, 12 predictor variables and three response variables. The three response variables are the amount of casual riders, the amount of registered riders, and the sum of these, total riders. 

#### Data Missingness

There is no missing data in the dataset, which provides some ease in the next step, Data Processing

```{r }
# Count missing data for each column
missing = as.data.frame(sapply(hourly, function(x) sum(is.na(x))))
DT::datatable(missing, colnames =c('Variable', 'Count of Missing'))


```


#### Two Routes of Analysis

As mentioned, there are two datasets involved in this analysis. The only difference between the two datasets is the presence of the 'hour' variable. This inherently creates 24 times the observations in the hourly dataset. I will use both datasets.

## Data Preparation



```{r  }


```

## Modeling

```{r  }


```

## Interpretation and Conclusion


```{r }

```

## Source






