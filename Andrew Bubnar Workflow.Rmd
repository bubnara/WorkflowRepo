---
title: "ISA 616 Workflow"
author: "Andrew Bubnar"
date: "10/1/2020"
output: 
  html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Optimizing Capital Bikeshare Product Distribution
##### An Analysis by Andrew Bubnar

## Overview
I believe that there is room to optimize the distribution process of Capital Bikeshare through predictive modeling. Capital Bikeshare is a bikesharing company, owned by Lyft Inc., that allows registered and casual users to rent public bikes in major cities around the world. Currently, bikes are deployed uniformly over some time dimensions. This creates a lapse in efficiency, that is, bikes are not always available when they need to be, and some bikes are sitting idle, producing no revenue. If we can predict the number of bikes that will be in use at any given time, we will maximize use, and thus profitability. 

The purpose of this analysis is, firstly, to deduce what factors impact the outcome of riders. The data that will be considered is primarily related to time, weather, and expected traffic. After concluding the impacts of each variable, suggestions to Capital Bikeshare's Logistics Controls team will be made (and passed along to distributors), to allow for a more satisfied customer base and increased profitabilty.

```{r, include = FALSE }
# Initialize all Packages that will be used
if(require(pacman)==FALSE)
  install.packages("pacman")

pacman::p_load(tidyverse, gtsummary, dplyr, DT, ggplot2, caret, corrplot, knitr)

```



## Data Selection, Description and Summary


#### Data Sourcing


The data involved was produced in conjunction by Capital Bikeshare and freemeteo.com. Two datasets were provided for analyses, differing on how they aggregate over time. One dataset has hourly data for ten independent variables with three dependent variables, while the other aggregates this same data by day. A data head, as well as a descriptive table defining each variable, can be found below:



#### Data Head & Codebook

```{r, echo = FALSE }
library(DT)
# Read in Data
daily = read.csv("day.csv")
hourly = read.csv("hour.csv")
head(hourly)

#Split data into training and validation sets
set.seed(42)
dailytrainindex = sample(1:nrow(daily), size = round(0.7*nrow(daily)), replace = FALSE)
daily.train = daily[dailytrainindex,]
daily.validation = daily[-dailytrainindex,]
hourlytrainindex = sample(1:nrow(hourly), size = round(0.7*nrow(daily)), replace = FALSE)
hourly.train = hourly[hourlytrainindex,]
hourly.validation = hourly[-hourlytrainindex,]

# Create data frame that holds descriptions
desc = as.data.frame(c('Observation #','Date','Season (1 = Spring, 2=Summer, etc.)','Year (0 = 2011, 1 = 2012)','Month (numeric)','Hour (0 = 12:00 am - 12:59 am, 23 = 11:00pm - 11:59 pm)','Public Holiday (Base 0 = no)', 'Day of Week (Sunday = 0)', 'Work Day (non-holiday, weekday)', 'Weather Condition (1 = Clear/Parly Cloudy, 2 = Mist/Cloudy, 3 = Light Snow/Rain, 4 = Heavy Rain/Snow/Thunderstorm','True Temperature (Normalized)','Feels Like Temperature (Normalized)','Windspeed (Normalized)','Humidity','Count of Unregistered Users','Count of Registered Users','Total Count of Users'),colnames(hourly), colnames='')
```

```{r }
# Display a Data Table to Display Descriptions
DT::datatable(desc, colnames="")
```

#### Data Structure

The columns are comprised of a primary key for each observation, the date, 12 predictor variables and three response variables. The three response variables are the amount of casual riders, the amount of registered riders, and the sum of these, total riders. 

#### Data Missingness

There is no missing data in the dataset, which provides some ease in the next step, Data Processing

```{r }
# Count missing data for each column
missing = as.data.frame(sapply(hourly, function(x) sum(is.na(x))))
DT::datatable(missing, colnames =c('Variable', 'Count of Missing'))


```


#### Two Routes of Analysis

As mentioned, there are two datasets involved in this analysis. The only difference between the two datasets is the presence of the 'hour' variable. This inherently creates 24 times the observations in the hourly dataset. I will use both datasets.

## Data Preparation

As this data was already preprocessed for the most part, as it is part of a competition dataset, there was not a lot of processing to do. The one thing that was necessary after importing the data into R was to make sure all the variables were of the right class. The following variables were converted to factors, from their original class of integer:

* Season
* Year
* Month
* Holiday
* Weekday
* Working Day

The other variables that needed to be considered for transformation were ID and Date. These two variables technically hold the same information, the ID field being in integer form, and the Date field being in Date String form. It is to be expected that there is a substantial daily, weekly and yearly seasonal component to the time series, but it is worth noting that there is also a substantial, positive trend component, as is show below. 

```{r  }
# Create a ggplot to display minimal continuous trend in Users over Time
df = ggplot(data = daily, aes(x=dteday, y=cnt, group = 10)) + geom_line(color="red") + labs(title = "Use Over Time", x = "Date", y = "Count of Users (daily)") + geom_smooth(method = "lm")
df
```
Due to this, I decided to use the ID variable as a count of days and removed the date variable. In the hourly dataset, I chose to alter the ID variable by dividing it by 24 and rounding up, resulting in the same value relative to date. This will allow me to include the trend component without having daily seasonality impede on the value of the variable. 

Coding the variables this way will now allow me to analyze each level of time (Hour, Weekday, Month, Year) as factors, and also as a continuous time series aggregated by day.

```{r, echo = FALSE }
# Convert Variables Weather Situation, Season, Year, Month, Holiday, Weekday and Working Day to factor

hourly$weathersit=as.factor(hourly$weathersit)
daily$weathersit =as.factor(daily$weathersit)
hourly$season = as.factor(hourly$season)
daily$season = as.factor(daily$season)
hourly$yr = as.factor(hourly$yr)
daily$yr = as.factor(daily$yr)
hourly$mnth = as.factor(hourly$mnth)
daily$mnth = as.factor(daily$mnth)
hourly$holiday = as.factor(hourly$holiday)
daily$holiday = as.factor(daily$holiday)
hourly$weekday = as.factor(hourly$weekday)
daily$weekday = as.factor(daily$weekday)
hourly$workingday = as.factor(hourly$workingday)
daily$workingday = as.factor(daily$workingday)

# Subset to remove Date and Index
daily = subset(daily, select = c(1,3:16))
names(daily)[1] = "day"
hourly = subset(hourly, select = c(1,3:17))
names(hourly)[1] = "day"
hourly$Day = ceiling(as.numeric(as.character(hourly$day)) / 24)
hourly$Day = as.integer(hourly$day)
```



## Modeling

The purpose of this model is to predict bike use on both an hourly and daily basis, and to determine which factors play substantial, scientifically meaningful roles at each level. To do this, I've conducted many predictive linear models that take into account different combinations of predictor variables. Ideally, the most competent model will be one that has the lowest `RMSE`, i.e. average error in predicted bike use for a given time. Another key statistic that I observed was the `coefficient of determination`, which determines the percentage of variation in bike use that is in direct response to changes in the predictors. Maximizing this will also ensure higher quality predictions.

For reference and understanding, I've included summary statistics for each variables simple linear regression model:


```{r, echo = FALSE }

#Split data into training and validation sets
set.seed(42)
dailytrainindex = sample(1:nrow(daily), size = round(0.7*nrow(daily)), replace = FALSE)
daily.train = daily[dailytrainindex,]
daily.validation = daily[-dailytrainindex,]
hourlytrainindex = sample(1:nrow(hourly), size = round(0.7*nrow(hourly)), replace = FALSE)
hourly.train = hourly[hourlytrainindex,]
hourly.validation = hourly[-hourlytrainindex,]

# Create simple linear regression models for each variable, get a simple understanding of usefulness and effects independently of others
mday = lm(daily.train$cnt~daily.train$day)
mseason = lm(daily.train$cnt~daily.train$season)
myear = lm(daily.train$cnt~daily.train$yr)
mmonth = lm(daily.train$cnt~daily.train$mnth)
mholiday = lm(daily.train$cnt~daily.train$holiday)
mweekday = lm(daily.train$cnt~daily.train$weekday)
mwork = lm(daily.train$cnt~daily.train$workingday)
mweather = lm(daily.train$cnt~daily.train$weathersit)
mtemp = lm(daily.train$cnt~daily.train$temp)
matemp = lm(daily.train$cnt~daily.train$atemp)
mhum = lm(daily.train$cnt~daily.train$hum)
mwind = lm(daily.train$cnt~daily.train$windspeed)
models = list(mday,mseason,myear, mmonth, mholiday,mweekday, mwork, mweather,mtemp, matemp, mhum, mwind)

# Print summary statistics in a table
library(broom)
glance = lapply(
  models, FUN = function(x) glance(x)[1:6]
  )
firstglance = lapply(
  models, FUN = function(x) glance(x)[1:6]
  )
glance = as.data.frame(matrix(unlist(glance), nrow = length(unlist(glance[6]))))
glance = t(glance)
DT::datatable(glance, colnames = c("R-Squared", "Adj. R-Squared", "Sigma", "T-Stat", "P-Value", "df"), rownames = colnames(daily[1:12]))


```

As you can see, there are some pretty useless independent variables in this dataset, such as "Weekday", "Working Day" and "Holiday". Knowing this made the process of creating my final model much easier.

## Final Model and Justification

```{r, echo = FALSE}
# Initialize models
model = lm(cnt~ day + season + weathersit + atemp + hum + windspeed + mnth, data = daily.train)
hourly.train$hr = as.factor(hourly.train$hr)
hourly.validation$hr = as.factor(hourly.validation$hr)
hourmodel = lm(hourly.train$cnt~hourly.train$hr)

# Calculate RMSE for Daily Model (doesn't apply to hourly model, that will just be used to see when to deploy)
library(forecast)
p.daily = predict(model, newdata=daily.validation)
modelacc = accuracy(p.daily, daily.validation$cnt)
RMSE = modelacc[2]
```

After building many models that considered all the variables available, as well as interactive and multiplicative variables, the final two models that will be presented to Capital Bikeshare are as follows:

* A simple linear regression model that predicts average user by time
* A multiple linear regression model that predicts the average users per day based on a combination of conditions


The summary statistics for the models as a whole are:

```{r  }
# Create Data Tables to Display Summary Stats
DT::datatable(as.data.frame(glance(hourmodel)[1:6]))

DT::datatable(as.data.frame(glance(model)[1:6],RMSE))

```

And the summary statistics for each individual variable within the models:

```{r, echo = FALSE}
library(gtsummary)
tbl_regression(hourmodel)
tbl_regression(model)

# Create a data frame that holds the coefficients of each hour
statdf = as.data.frame(hourmodel$coefficients)
```

Below you can see the the impact of hour of the day independent from other variables:

```{r  }
# Plot displays impact of hour of the day on users
ggplot(data = statdf, mapping = aes(x = seq(1,length(hourmodel$coefficients)), y = hourmodel$coefficients)) + geom_bar(position = "dodge", stat = "identity")+ xlab("Hour of Day") + ylab ("Coefficient") + ggtitle("Impact of Hour of Day on Users")
```

#### Justification

The two models created above, when used together, hold extensive power in deciding when Capital Bikeshare should be deploying their bikes for optimal use and warehousing. 

The first model, which is a simple linear regression that predicts users by hour, has a model r-squared of ~.5 and a p-value of 0. This concluded that 50% `YES! 50%` of the variation in users can be attributed to the hour of the day. This is somewhat intuitive, as more people would use the service during the day than at night, but regardless, one variable predicting 50% variation is incredible! The graph above displays clearly, there are times of the day with substantial variation hour to hour. Specifically, between 1 am and 2 am, and between 6 am and 7 am, there is an great decrease, and a great increase in riders. We will touch on this later.

The second model, which uses a multiple linear regression format, maintains an adjusted r-squared value of ~.816 and an RMSE value of ~730. This means that 81.6% of the variation in bike use can be attributed to the predictive factors in the model, and that on average, each day, the model misses bike use by ~730 users. This may seem like a big number, but when put into the terms of over 4500 users per day, and a 50% chance of predictions being more than ample, there is only an 8% chance that bikes are underutilized in comparison to their potential each day.


## Interpretation and Conclusion

My suggestion to Capital BikeShare is twofold:

Firstly, deploy the amount of bikes that the predictive model suggests each day. This will be somewhat routine, as a lot of the variation is seasonal, and thus, staffing hours can be adjusted to accomodate for these seasonal changes. 

Secondly, under `absolutely no circumstances` should each days bikes not be deployed by 7 am, and not be picked up after 1 am. This will ensure that when the morning spike takes place, there will not be a shortage of bikes to start the day. Second, it will ensure that bikes are not idle through the late hours of the night, prone to damage and theft. 

If Capital Bikeshare manages to implement these two suggestions, cost savings and revenue increases will be certain.
